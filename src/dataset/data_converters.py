"""
Data Converters for LLM Adapter

This module handles data conversion between LLM data formats and 
naive_pll compatible formats.
"""
import logging
import pandas as pd
import torch
import operator
import numpy as np
from functools import reduce
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from src.dataset.datasets import SatoCVTablewiseDataset, SatoCVColwiseDataset
from src.llm.data_utils import TableInstance
import random

logger = logging.getLogger(__name__)


@dataclass
class PllDataInstance:
    """
    Represents a single column data instance for partial label learning.
    This is the output of the LLM Bootstrapping phase.

    Attributes:
        table_id: Unique identifier of the source table
        column_name: Name of the column (header)
        column_features: Feature representation of the column. In the initial phase,
                        this can be the serialized string used by watchog, or raw pd.Series
        candidate_set: List of candidate labels (types) generated by LLM
        col_idx: Column index within the table (0-based)
    """
    table_id: str
    column_name: str
    column_features: Any
    candidate_set: List[str]
    col_idx: int
@dataclass
class SingleColumnPllInstance:
    """
    Represents a single column data instance for single-column PLL learning.
    This is optimized for individual column prediction without table context.

    Attributes:
        table_id: Unique identifier of the source table
        column_name: Name of the column (header)
        column_data: Raw column data as string (for single column prediction)
        candidate_labels: List of candidate labels (types) generated by LLM
        true_label: True label for this column
        col_idx: Column index within the table (0-based)
    """
    table_id: str
    column_name: str
    column_data: str
    candidate_labels: List[str]
    col_idx: int

class DataConverter:
    """Base class for data conversion operations."""
    
    def __init__(self, tokenizer, max_length: int = 128, device=None, config=None):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.config = config

    def convert_table_instances_to_dataset(self, table_instances: List[TableInstance]):
        """
        Convert LLM table instances to SatoCVTablewiseDataset format.
        
        Args:
            table_instances: List of LLM table instances
            
        Returns:
            Dataset compatible with SatoCVTablewiseDataset
        """
        logger.info(f"Converting {len(table_instances)} table instances to SatoCVTablewiseDataset format")
        # Create and return the custom dataset
        return TablewiseDataset(
            table_instances=table_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )
    
    def convert_to_single_column_dataset(self, table_instances: List[TableInstance]):
        """
        Convert LLM table instances to SatoCVColwiseDataset format for single column prediction.
        
        Args:
            table_instances: List of LLM table instances
        
        Returns:
            Dataset compatible with SatoCVColwiseDataset
        """
        logger.info(f"Converting {len(table_instances)} table instances to SatoCVColwiseDataset format")
        # Create and return the custom dataset
        return ColwiseDataset(
            table_instances=table_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )
    
    def convert_pll_instances_to_dataset(self, pll_instances: List[PllDataInstance], type_ontology: List[str]):
        """
        Convert PLL data instances to format compatible with naive_pll.
        
        Args:
            pll_instances: List of PLL data instances
            type_ontology: List of type labels from ontology
            
        Returns:
            Dataset compatible with naive_pll
        """
        logger.info(f"Converting {len(pll_instances)} PLL instances to dataset format")
        
        # Create and return the custom dataset
        return MultiPLLDataset(
            pll_instances=pll_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            type_ontology=type_ontology,
            config=self.config
        )
    
    def convert_single_column_instances_to_dataset(self, single_column_instances: List[SingleColumnPllInstance], type_ontology: List[str]):
        """
        Convert single column PLL instances to dataset format compatible with doduo_single trainer.
        
        Args:
            single_column_instances: List of SingleColumnPllInstance objects
            type_ontology: Available type vocabulary
            
        Returns:
            Dataset compatible with doduo_single trainer
        """
        logger.info(f"Converting {len(single_column_instances)} single column instances to dataset format...")
        
        # Create a custom dataset that extends SatoCVColwiseDataset        
        return SinglePLLDataset(
            single_column_instances, 
            self.tokenizer, 
            type_ontology,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )

# src/dataset/data_converters_proden.py

import torch
import numpy as np
import pandas as pd
import operator
from functools import reduce
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)


class MultiPLLDataset:
    """
    Multi-column PLL Dataset for ProDEN with Augmentation (Back-Translation / Shuffle+Dropout).

    改动：
      - 增加 input_ids_bt (augment view): 对列内元素进行 Shuffle 和 Dropout。
      - 增加 cls_indexes_bt: 增强视图对应的 CLS 位置。
    """

    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config, threshold_ratio=0.01):
        self.type_ontology = type_ontology

        if all(hasattr(inst, "table") for inst in instances):
            instances = self._convert_table_instances(instances)

        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.model_name = config.model_name.lower()

        # 是否启用 AUM 模式
        self.use_aum = bool(getattr(config, "use_aum", False) and threshold_ratio > 0)
        print(f"[AUM] Use AUM mode: {self.use_aum}")
        
        # ==========================================================
        # 类别数 +1 (为 AUM 预留一个 "Noise" 类别)
        self.num_real_classes = len(type_ontology)
        self.noise_class_id = self.num_real_classes if self.use_aum else None
        self.threshold_ratio = threshold_ratio if self.use_aum else 0.0
        # self.num_classes = self.num_real_classes + (1 if self.use_aum else 0)
        # ==========================================================

        # 按 table_id 分组
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)

        data_list = []
        global_col_counter = 0  # 全局列计数器

        # 用于后续分析，记录哪些 ID 被选为了阈值样本
        self.threshold_sample_ids = []
        
        for table_id, table_pll_instances in tqdm(table_groups.items(), desc="Building MultiPLL Dataset"):
            # 保持列顺序
            table_pll_instances.sort(key=lambda x: x.col_idx)
            
            if len(table_pll_instances) > 10:
                continue

            token_ids_list = []
            token_ids_list_bt = []
            candidate_labels_per_col = []
            weak_label_ids = []
            class_ids = []
            col_indices = []
            global_col_indices = []

            is_threshold_flags = []

            for pll_instance in table_pll_instances:
                # 1. 获取原始列数据 (List)
                if hasattr(pll_instance.column_features, 'tolist'):
                    col_data_raw = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    col_data_raw = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    col_data_raw = [str(pll_instance.column_features)]

                # 2. 生成原始视图 Token
                tokens_orig = self._get_token_ids(col_data_raw, augment=False)
                token_ids_list.append(tokens_orig)

                # 3. 生成增强视图 Token (Shuffle + Dropout) -> input_ids_bt
                #    注意：增强视图只用于计算一致性 Loss，不用于 GT Label 匹配，但顺序要和 columns 对应
                tokens_bt = self._get_token_ids(col_data_raw, augment=True)
                token_ids_list_bt.append(tokens_bt)

                # 4. 标签处理 (保持不变)
                cand_ids = [self._label_to_class_id(label) for label in getattr(pll_instance, "candidate_set", []) if label is not None]
                if len(cand_ids) == 0:
                    # Fallback: 没有候选时，用真值填充，避免弱标签为空
                    cand_ids = [self._label_to_class_id(pll_instance.column_name)]
                candidate_labels_per_col.append(cand_ids)
                
                true_class_id = self._label_to_class_id(pll_instance.column_name)
                weak_class_id = cand_ids[0]
                # ==========================================================
                # 掷骰子决定是否作为 Threshold Sample（仅 AUM 模式下启用）
                # 逻辑：如果是阈值样本，强制把 GT 改为 Noise Class
                is_threshold = False
                if self.use_aum and self.threshold_ratio > 0 and random.random() < self.threshold_ratio:
                    weak_class_id = self.noise_class_id # 强制改为 N+1 类
                    is_threshold = True
                    self.threshold_sample_ids.append(global_col_counter)
                
                weak_label_ids.append(weak_class_id)
                class_ids.append(true_class_id)
                is_threshold_flags.append(1 if is_threshold else 0)
                # ==========================================================
                
                col_indices.append(pll_instance.col_idx)
                global_col_indices.append(global_col_counter)
                global_col_counter += 1

            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)
            cls_indexes = self._calculate_cls_indexes(token_ids_list).to(device)
            token_ids_bt = torch.LongTensor(reduce(operator.add, token_ids_list_bt)).to(device)
            cls_indexes_bt = self._calculate_cls_indexes(token_ids_list_bt).to(device)

            weak_label_tensor = torch.LongTensor(weak_label_ids).to(device)
            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            col_indices_tensor = torch.LongTensor(col_indices).to(device)
            global_col_indices_tensor = torch.LongTensor(global_col_indices).to(device)
            is_threshold_tensor = torch.LongTensor(is_threshold_flags).to(device)   # Added

            data_list.append([
                table_id,
                len(table_pll_instances),
                token_ids,              # input_ids
                token_ids_bt,           # input_ids_bt [新增]
                weak_label_tensor,
                class_ids_tensor,
                cls_indexes,            # cls_indexes
                cls_indexes_bt,         # cls_indexes_bt [新增]
                candidate_labels_per_col,
                col_indices_tensor,
                global_col_indices_tensor,
                is_threshold_tensor # [新增] 用于 Trainer 区分                
            ])
        
        self.table_df = pd.DataFrame(
            data_list,
            columns=[
                "table_id",
                "num_col",
                "data_tensor",      # input_ids
                "data_tensor_bt",   # input_ids_bt [新增]
                "weak_label_tensor",
                "label_tensor",
                "cls_indexes",
                "cls_indexes_bt",   # [新增]
                "candidate_labels",
                "col_indices",
                "global_col_indices",
                "is_threshold" # [新增]                
            ],
        )

        # 记录总样本数供 Trainer 使用
        self.num_samples = global_col_counter
        print(f"[Dataset] Built with {len(self.table_df)} tables, {global_col_counter} cols.")
        if self.use_aum:
            print(f"[AUM] Created {len(self.threshold_sample_ids)} threshold samples (Label={self.noise_class_id}).")

    def _get_token_ids(self, col_data_list: List[str], augment: bool = False) -> List[int]:
        """
        序列化并 Tokenize。
        Augment 策略: Random Shuffle + 15% Dropout
        """
        # 复制数据，避免修改原引用
        vals = col_data_list[:]
        
        if augment and len(vals) > 0:
            # 1. Shuffle
            random.shuffle(vals)
            # 2. Dropout (保留 85%)
            if len(vals) > 1:
                keep_n = max(1, int(len(vals) * 0.85))
                vals = vals[:keep_n]
        
        col_str = " ".join(vals) if vals else ""
        
        # Tokenize
        token_ids = self.tokenizer.encode(
            col_str,
            add_special_tokens=True, # 自动加 [CLS] ... [SEP]
            max_length=self.max_length + 2,
            truncation=True
        )
        return token_ids

    def _calculate_cls_indexes(self, token_ids_list: List[List[int]]) -> torch.Tensor:
        """根据 Token 列表长度计算 CLS 位置"""
        lengths = np.array([len(x) for x in token_ids_list])
        
        if "bert" in self.model_name:
            # BERT: CLS 在开头
            # [CLS] A [SEP], [CLS] B [SEP] ...
            # 0, len(A), len(A)+len(B)...
            cls_indices = [0] + np.cumsum(lengths).tolist()[:-1]
            return torch.LongTensor(cls_indices)
            
        elif "qwen" in self.model_name:
            # Causal LM: 这里的 pooling 通常取最后一个 token
            # len(A)-1, len(A)+len(B)-1 ...
            last_indices = np.cumsum(lengths) - 1
            return torch.LongTensor(last_indices)
        else:
            # 默认按 BERT 处理
            cls_indices = [0] + np.cumsum(lengths).tolist()[:-1]
            return torch.LongTensor(cls_indices)

    def _label_to_class_id(self, label):
        # 允许 label 传入为 str 或 int（索引）
        if isinstance(label, (int, np.integer)):
            return int(label) if 0 <= int(label) < len(self.type_ontology) else 0
        try:
            label_index = self.type_ontology.index(str(label))
            return label_index
        except ValueError:
            # 如果 label 是数字形式的字符串，尝试解析
            if isinstance(label, str) and label.isdigit():
                idx = int(label)
                return idx if 0 <= idx < len(self.type_ontology) else 0
            return 0

    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]

        return {
            "data": row["data_tensor"],              # input_ids
            "data_bt": row["data_tensor_bt"],        # input_ids_bt [新增]
            "weak_labels": row["weak_label_tensor"].flatten(),
            "labels": row["label_tensor"].flatten(),
            "is_threshold": row["is_threshold"].flatten(),
            "table_id": row["table_id"],
            "col_idx": row["col_indices"],
            "global_col_indices": row["global_col_indices"],
            "cls_indexes": row["cls_indexes"],
            "cls_indexes_bt": row["cls_indexes_bt"], # [新增]
            "candidate_labels": row["candidate_labels"],
        }

    def _convert_table_instances(self, table_instances: List[TableInstance]) -> List[PllDataInstance]:
        """将 TableInstance 转为 PllDataInstance，便于与训练数据一致处理。"""
        pll_list = []
        for table_instance in table_instances:
            table_df = table_instance.table
            labels = getattr(table_instance, "labels", {}) or {}
            col_indices = getattr(table_instance, "col_indices", None)

            # 若未提供 col_indices，按顺序生成
            if not col_indices or len(col_indices) != len(table_df.columns):
                col_indices = list(range(len(table_df.columns)))

            for idx, col_name in enumerate(table_df.columns):
                label_val = labels.get(col_name)
                label_text = self._normalize_label_value(label_val)
                # 若缺失标签，用列名占位，避免空候选
                column_label = label_text if label_text is not None else col_name
                candidate_set = [column_label] if column_label is not None else []

                pll_list.append(PllDataInstance(
                    table_id=table_instance.table_id,
                    column_name=column_label,
                    column_features=table_df[col_name],
                    candidate_set=candidate_set,
                    col_idx=col_indices[idx]
                ))
        return pll_list

    def _normalize_label_value(self, label_val):
        """将标签值统一为 ontology 中的名字，支持数字索引/字符串。"""
        if label_val is None:
            return None
        # 数字索引
        if isinstance(label_val, (int, np.integer)):
            return self.type_ontology[label_val] if 0 <= label_val < len(self.type_ontology) else str(label_val)
        # 字符串索引
        if isinstance(label_val, str) and label_val.isdigit():
            idx = int(label_val)
            return self.type_ontology[idx] if 0 <= idx < len(self.type_ontology) else label_val
        return str(label_val)




class _MultiPLLDataset:
    """
    Multi-column PLL Dataset for ProDEN.

    与原 MultiPLLDataset 基本一致，区别：
      - 去掉 PoP 用的 candidate_distributions / candidate_mask 逻辑；
      - 额外为每一列分配一个全局列索引 global_col_indices，方便 Trainer 维护全局 Q。
    """

    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config):
        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.type_ontology = type_ontology

        # 统计总列数用
        self.num_classes = len(type_ontology)

        # 按 table_id 分组
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)

        data_list = []
        global_col_counter = 0  # 全局列计数器

        for table_id, table_pll_instances in tqdm(table_groups.items(), desc="Building ProDEN MultiPLLDataset"):
            # 保持列顺序
            table_pll_instances.sort(key=lambda x: x.col_idx)

            # 你原代码里有 “len(table_pll_instances) > 10 就跳过” 的防显存逻辑，这里保持
            if len(table_pll_instances) > 10:
                continue

            token_ids_list = []
            candidate_labels_per_col = []
            class_ids = []
            col_indices = []
            global_col_indices = []

            for pll_instance in table_pll_instances:
                # 列值文本构造
                if hasattr(pll_instance.column_features, 'tolist'):
                    col_data = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    col_data = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    col_data = [str(pll_instance.column_features)]
                col_str = " ".join(col_data) if col_data else ""

                # tokenize
                token_ids = tokenizer.encode(
                    col_str,
                    add_special_tokens=True,
                    max_length=max_length + 2,
                    truncation=True
                )
                token_ids_list.append(token_ids)

                # 候选标签 -> class id
                cand_ids = [self._label_to_class_id(label) for label in pll_instance.candidate_set]
                candidate_labels_per_col.append(cand_ids)
                
                # 真标签（用于 eval）
                true_class_id = self._label_to_class_id(pll_instance.column_name)
                class_ids.append(true_class_id)

                # 当前列在表里的 col_idx
                col_indices.append(pll_instance.col_idx)

                # 分配全局列索引
                global_col_indices.append(global_col_counter)
                global_col_counter += 1

            # 把同一张表所有列的 token 串起来
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)

            # CLS 位置（按你的 config 的模型类型选择）
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]

            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1

            if "bert" in config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(device)
            elif "qwen" in config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(device)
            else:
                raise ValueError(f"Unsupported model: {config.model_name}")

            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            col_indices_tensor = torch.LongTensor(col_indices).to(device)
            global_col_indices_tensor = torch.LongTensor(global_col_indices).to(device)
            
            data_list.append([
                table_id,
                len(table_pll_instances),   # num_col
                token_ids,
                class_ids_tensor,
                cls_indexes,
                candidate_labels_per_col,
                col_indices_tensor,
                global_col_indices_tensor,
            ])

        self.table_df = pd.DataFrame(
            data_list,
            columns=[
                "table_id",
                "num_col",
                "data_tensor",
                "label_tensor",
                "cls_indexes",
                "candidate_labels",
                "col_indices",
                "global_col_indices",
            ],
        )

        logger.info(f"[ProDEN] Built dataset with {len(self.table_df)} tables, "
                    f"{global_col_counter} columns total.")

    def _label_to_class_id(self, label):
        """Convert label string to class ID using type ontology."""
        try:
            label_index = self.type_ontology.index(label)
            return label_index
        except ValueError:
            logger.warning(f"Label '{label}' not found in type ontology, using default ID 0")
            return 0

    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]

        return {
            "data": row["data_tensor"],
            "label": row["label_tensor"].flatten(),          # [num_cols]
            "table_id": row["table_id"],
            "col_idx": row["col_indices"],                  # [num_cols]
            "global_col_indices": row["global_col_indices"],# [num_cols] 全局列索引
            "cls_indexes": row["cls_indexes"],              # [num_cols]
            "candidate_labels": row["candidate_labels"],    # List[List[int]]
        }

class _MultiPLLDataset:
    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config):
        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.type_ontology = type_ontology
        # Store progressively refined soft candidate distributions per (table_id, col_idx)
        # Value shape: [num_classes] float tensor on CPU
        self._candidate_distributions = {}
        
        # Group PLL instances by table_id
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)
        
        # Convert grouped PLL instances to DataFrame format
        data_list = []
        for table_id, table_pll_instances in tqdm(table_groups.items()):
            # Sort by col_idx to maintain order
            table_pll_instances.sort(key=lambda x: x.col_idx)
            
            # 避免爆显存
            if len(table_pll_instances) > 10:
                continue
            
            # Tokenize table data
            token_ids_list = []
            candidate_labels = []
            class_ids = []
            col_indices = []
            
            for pll_instance in table_pll_instances:
                # Get column data from column_features
                if hasattr(pll_instance.column_features, 'tolist'):
                    # If it's a pandas Series
                    col_data = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    # If it's already a list
                    col_data = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    # If it's a string representation
                    col_data = [str(pll_instance.column_features)]
                
                col_str = " ".join(col_data) if col_data else ""
                
                token_ids = tokenizer.encode(
                    col_str, 
                    add_special_tokens=True, 
                    max_length=max_length + 2
                )
                token_ids_list.append(token_ids)
                
                # Convert candidate labels to class IDs
                candidate_class_ids = [self._label_to_class_id(label) for label in pll_instance.candidate_set]
                candidate_labels.append(candidate_class_ids)
                
                # Use column_name as the true label
                true_class_id = self._label_to_class_id(pll_instance.column_name)
                class_ids.append(true_class_id)
                
                # Collect col_idx
                col_indices.append(pll_instance.col_idx)
            
            # Combine all token IDs
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)
            
            # Calculate CLS indexes
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]

            # Calculate last token index
            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1
            
            if "bert" in config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(device)
            elif "qwen" in config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(device)
            else:
                raise ValueError(f"Unsupported model: {config.model_name}")
            
            # cls_indexes = torch.LongTensor(cls_index_list).to(device)
            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            
            data_list.append([
                table_id,
                len(table_pll_instances),  # num_col
                token_ids,
                class_ids_tensor,
                cls_indexes,
                candidate_labels,
                col_indices,
            ])
        
        # Create DataFrame in SatoCVTablewiseDataset format
        self.table_df = pd.DataFrame(
            data_list,
            columns=["table_id", "num_col", "data_tensor", "label_tensor", "cls_indexes", "candidate_labels", "col_indices"]
        )
    
    def update_candidate_masks(self, updates: list) -> None:
        """Update internal soft candidate distributions with PRODEN-style refined masks.
        Each update item is a dict with keys: 'table_id', 'col_idx', 'updated_mask' (Tensor [C]).
        """
        if updates is None:
            return
        for item in updates:
            table_id = item.get('table_id')
            col_idx = int(item.get('col_idx')) if item.get('col_idx') is not None else None
            updated_mask = item.get('updated_mask')
            if table_id is None or col_idx is None or updated_mask is None:
                continue
            # Ensure CPU float tensor
            if isinstance(updated_mask, torch.Tensor):
                mask = updated_mask.detach().to('cpu').float()
            else:
                mask = torch.tensor(updated_mask, dtype=torch.float32)
            self._candidate_distributions[(table_id, col_idx)] = mask
    
    def get_candidate_distribution(self, table_id: str, col_idx: int):
        """Return refined soft distribution for (table_id, col_idx) if available, else None."""
        return self._candidate_distributions.get((table_id, int(col_idx)))
    
    def _label_to_class_id(self, label):
        """Convert label string to class ID using type ontology."""
        # Use the loaded type ontology to map labels to IDs
        try:
            # Find the index of the label in the type ontology
            label_index = self.type_ontology.index(label)
            return label_index
        except ValueError:
            # If label not found in ontology, return 0 (default)
            logger.warning(f"Label '{label}' not found in type ontology, using default ID 0")
            return 0
    
    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]
        col_indices = torch.LongTensor(row["col_indices"])
        table_id = row["table_id"]
        
        # get soft candidate mask dynamically
        refined_mask = []
        for i, col_idx in enumerate(col_indices.tolist()):
            soft_mask = self.get_candidate_distribution(table_id, col_idx)
            if soft_mask is not None:
                refined_mask.append(soft_mask)
            else:
                mask = torch.zeros(len(self.type_ontology))
                for label in row["candidate_labels"][i]:
                    mask[label] = 1
                refined_mask.append(mask)
        refined_mask = torch.stack(refined_mask)
        
        return {
            "data": row["data_tensor"],
            "label": row["label_tensor"].flatten(),
            "table_id": table_id,
            "col_idx": col_indices,
            "cls_indexes": row["cls_indexes"],
            "candidate_labels": row["candidate_labels"],
            "candidate_mask": refined_mask
        }
    
    # def __getitem__(self, idx: int) -> dict:
    #     row = self.table_df.iloc[idx]
    #     # Use the saved col_idx from PLL instances instead of manually calculating
    #     col_indices = torch.LongTensor(row["col_indices"])
        
    #     return {
    #         "data": row["data_tensor"],
    #         "label": row["label_tensor"].flatten(),  # Flatten multi-column labels
    #         "table_id": row["table_id"],
    #         "candidate_labels": row["candidate_labels"],
    #         "col_idx": col_indices,  # Column indices from original data
    #         "cls_indexes": row["cls_indexes"]
    #     }

class SinglePLLDataset(SatoCVColwiseDataset):
    def __init__(self, single_column_instances, tokenizer, type_ontology, max_length=128, device=None, config=None):
        self.single_column_instances = single_column_instances
        self.tokenizer = tokenizer
        self.type_ontology = type_ontology
        self.max_length = max_length
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device
        
        # Process single column instances into the format expected by SatoCVColwiseDataset
        self._process_single_column_instances()
    
    def _process_single_column_instances(self):
        """Process single column PLL instances into SatoCVColwiseDataset format"""
        
        row_list = []
        for instance in self.single_column_instances:
            # Convert candidate labels to label IDs
            candidate_label_ids = []
            for label in instance.candidate_labels:
                if label in self.type_ontology:
                    label_id = self.type_ontology.index(label)
                    candidate_label_ids.append(label_id)
            
            # Get true label ID
            true_label_id = 0
            if instance.column_name in self.type_ontology:
                true_label_id = self.type_ontology.index(instance.column_name)
            
            # Create row data similar to SatoCVColwiseDataset format
            row_data = {
                'table_id': instance.table_id,
                'col_idx': instance.col_idx,
                'col_name': instance.column_name,
                'class_id': true_label_id,
                'data': instance.column_data,
                'candidate_labels': candidate_label_ids  # Add candidate labels
            }
            row_list.append(row_data)
        
        # Create DataFrame
        self.df = pd.DataFrame(row_list)
        
        # Convert into torch.Tensor (similar to SatoCVColwiseDataset)
        self.df["data_tensor"] = self.df["data"].apply(
            lambda x: torch.LongTensor(
                self.tokenizer.encode(x,
                                    add_special_tokens=True,
                                    max_length=self.max_length + 2)).to(self.device))
        self.df["label_tensor"] = self.df["class_id"].apply(
            lambda x: torch.LongTensor([x]).to(self.device)
        )
        # Store progressively refined soft candidate distributions per (table_id, col_idx)
        # Value shape: [num_classes] float tensor on CPU
        self._candidate_distributions = {}
    
    def __len__(self) -> int:
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {
            'data': row["data_tensor"],  # Use 'data' key for compatibility with doduo_single_trainer
            'label': row["label_tensor"],
            'table_id': row["table_id"],
            'col_idx': row["col_idx"],
            'col_name': row["col_name"],
            'candidate_labels': row["candidate_labels"]  # Include candidate labels
        }
    
    def update_candidate_masks(self, updates: list) -> None:
        """Update internal soft candidate distributions with PRODEN-style refined masks.
        Each update item is a dict with keys: 'table_id', 'col_idx', 'updated_mask' (Tensor [C]).
        """
        if updates is None:
            return
        for item in updates:
            table_id = item.get('table_id')
            col_idx = int(item.get('col_idx')) if item.get('col_idx') is not None else None
            updated_mask = item.get('updated_mask')
            if table_id is None or col_idx is None or updated_mask is None:
                continue
            # Ensure CPU float tensor
            if isinstance(updated_mask, torch.Tensor):
                mask = updated_mask.detach().to('cpu').float()
            else:
                mask = torch.tensor(updated_mask, dtype=torch.float32)
            self._candidate_distributions[(table_id, col_idx)] = mask

    def get_candidate_distribution(self, table_id: str, col_idx: int):
        """Return refined soft distribution for (table_id, col_idx) if available, else None."""
        return self._candidate_distributions.get((table_id, int(col_idx)))

# Create a custom dataset that wraps SatoCVTablewiseDataset
# but uses LLM data instead of loading from CSV
class TablewiseDataset:
    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config=None):
        self.table_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.type_ontology = type_ontology
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device

        self._process_table_instances()

    def _label_to_class_id(self, label):
        label_index = self.type_ontology.index(label)
        return label_index

    def _process_table_instances(self):        
        data_list = []
        for table_instance in tqdm(self.table_instances):
            table_df = table_instance.table
            labels = table_instance.labels

            # 避免爆显存
            if len(table_instance.table.columns) > 10:
                continue
            
            # Handle col_indices - use saved ones if available, otherwise generate them
            if hasattr(table_instance, 'col_indices') and table_instance.col_indices is not None:
                col_indices = table_instance.col_indices
            else:
                # Generate column indices for tables without saved col_indices
                col_indices = list(range(len(table_df.columns)))
            
            # Convert table data to token IDs (similar to SatoCVTablewiseDataset)
            token_ids_list = []
            candidate_labels = []
            for col_name in table_df.columns:
                # Get column data and convert to string
                try:
                    col_data = table_df[col_name].dropna().astype(str).tolist()
                except:
                    print(table_df[col_name])
                col_str = " ".join(col_data) if col_data else ""
                
                token_ids = self.tokenizer.encode(
                    col_str, 
                    add_special_tokens=True, 
                    max_length=self.max_length + 2
                )
                token_ids_list.append(token_ids)
                # Convert candidate labels to class IDs
                # candidate_class_ids = [self._label_to_class_id(label) for label in table_instance["candidate_set"]]
                # candidate_labels.append(candidate_class_ids)
            
            # Combine all token IDs
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(self.device)
            
            # Calculate CLS indexes
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]
            
            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1
            
            if "bert" in self.config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(self.device)
            elif "qwen" in self.config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(self.device)
            else:
                raise ValueError(f"Unsupported model: {self.config.model_name}")
            
            # Convert labels to class IDs
            # labels is a dict mapping column names to class IDs
            class_ids = torch.LongTensor([labels.get(col_name, 0) for col_name in table_df.columns]).to(self.device)
            
            data_list.append([
                table_instance.table_id,
                len(table_df.columns),  # num_col
                token_ids,
                class_ids,
                cls_indexes,
                # candidate_labels,
                col_indices
            ])
        
        # Create DataFrame in SatoCVTablewiseDataset format
        self.table_df = pd.DataFrame(
            data_list,
            columns=["table_id", "num_col", "data_tensor", "label_tensor", "cls_indexes", 
            # "candidate_labels", 
            "col_indices"]
        )
    
    def __len__(self) -> int:
        return len(self.table_df)
    
    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]
        # Use the saved col_indices from original data instead of manually calculating
        col_indices = torch.LongTensor(row["col_indices"])
        
        return {
            "data": row["data_tensor"],
            "labels": row["label_tensor"],
            # "weak_labels": None,
            "table_id": row["table_id"],
            "col_idx": col_indices,  # Column indices from original data
            # "candidate_labels": row["candidate_labels"],  # Standard method doesn't use candidate labels
            "candidate_labels": None,
            "cls_indexes": row["cls_indexes"]
        }

class ColwiseDataset(SatoCVColwiseDataset):
    def __init__(self, table_instances, tokenizer, max_length=128, device=None, config=None):
        self.table_instances = table_instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device
        
        # Process table instances into the format expected by SatoCVColwiseDataset
        self._process_table_instances()

    def _process_table_instances(self):
        """Process LLM table instances into SatoCVColwiseDataset format"""
        
        row_list = []
        for table_instance in self.table_instances:
            table_df = table_instance.table
            labels = table_instance.labels
            
            # Process each column as a separate row
            for col_idx, col_name in enumerate(table_df.columns):
                # Get column data and convert to string
                col_data = table_df[col_name].dropna().astype(str).tolist()
                col_str = " ".join(col_data) if col_data else ""
                
                # Get label for this column
                label_id = labels.get(col_name, 0)
                
                # Create row data similar to SatoCVColwiseDataset format
                row_data = {
                    'table_id': table_instance.table_id,
                    'col_idx': col_idx,
                    'col_name': col_name,
                    'class_id': label_id,
                    'data': col_str
                }
                row_list.append(row_data)
        
        # Create DataFrame
        self.df = pd.DataFrame(row_list)
        
        # Convert into torch.Tensor (similar to SatoCVColwiseDataset)
        self.df["data_tensor"] = self.df["data"].apply(
            lambda x: torch.LongTensor(
                self.tokenizer.encode(x,
                                    add_special_tokens=True,
                                    max_length=self.max_length + 2)).to(self.device))
        self.df["label_tensor"] = self.df["class_id"].apply(
            lambda x: torch.LongTensor([x]).to(self.device)
        )

    def __len__(self) -> int:
        return len(self.df)
    
    def __getitem__(self, idx: int) -> dict:
        return {
            "data": self.df.iloc[idx]["data_tensor"],
            "label": self.df.iloc[idx]["label_tensor"],
            "candidate_labels": None  # Single column method doesn't use candidate labels
        }