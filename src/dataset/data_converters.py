"""
Data Converters for LLM Adapter

This module handles data conversion between LLM data formats and 
naive_pll compatible formats.
"""
import logging
import pandas as pd
import torch
import operator
import numpy as np
from functools import reduce
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from src.dataset.datasets import SatoCVTablewiseDataset, SatoCVColwiseDataset
from src.llm.data_utils import TableInstance
import random

logger = logging.getLogger(__name__)


@dataclass
class PllDataInstance:
    """
    Represents a single column data instance for partial label learning.
    This is the output of the LLM Bootstrapping phase.

    Attributes:
        table_id: Unique identifier of the source table
        column_name: Name of the column (header)
        column_features: Feature representation of the column. In the initial phase,
                        this can be the serialized string used by watchog, or raw pd.Series
        candidate_set: List of candidate labels (types) generated by LLM
        col_idx: Column index within the table (0-based)
    """
    table_id: str
    column_name: str
    column_features: Any
    candidate_set: List[str]
    col_idx: int
@dataclass
class SingleColumnPllInstance:
    """
    Represents a single column data instance for single-column PLL learning.
    This is optimized for individual column prediction without table context.

    Attributes:
        table_id: Unique identifier of the source table
        column_name: Name of the column (header)
        column_data: Raw column data as string (for single column prediction)
        candidate_labels: List of candidate labels (types) generated by LLM
        true_label: True label for this column
        col_idx: Column index within the table (0-based)
    """
    table_id: str
    column_name: str
    column_data: str
    candidate_labels: List[str]
    col_idx: int

class DataConverter:
    """Base class for data conversion operations."""
    
    def __init__(self, tokenizer, max_length: int = 128, device=None, config=None):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.config = config

    def convert_table_instances_to_dataset(self, table_instances: List[TableInstance]):
        """
        Convert LLM table instances to SatoCVTablewiseDataset format.
        
        Args:
            table_instances: List of LLM table instances
            
        Returns:
            Dataset compatible with SatoCVTablewiseDataset
        """
        logger.info(f"Converting {len(table_instances)} table instances to SatoCVTablewiseDataset format")
        # Create and return the custom dataset
        return TablewiseDataset(
            table_instances=table_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )
    
    def convert_to_single_column_dataset(self, table_instances: List[TableInstance]):
        """
        Convert LLM table instances to SatoCVColwiseDataset format for single column prediction.
        
        Args:
            table_instances: List of LLM table instances
        
        Returns:
            Dataset compatible with SatoCVColwiseDataset
        """
        logger.info(f"Converting {len(table_instances)} table instances to SatoCVColwiseDataset format")
        # Create and return the custom dataset
        return ColwiseDataset(
            table_instances=table_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )
    
    def convert_pll_instances_to_dataset(self, pll_instances: List[PllDataInstance], type_ontology: List[str]):
        """
        Convert PLL data instances to format compatible with naive_pll.
        
        Args:
            pll_instances: List of PLL data instances
            type_ontology: List of type labels from ontology
            
        Returns:
            Dataset compatible with naive_pll
        """
        logger.info(f"Converting {len(pll_instances)} PLL instances to dataset format")
        
        # Create and return the custom dataset
        return MultiPLLDataset(
            pll_instances=pll_instances,
            tokenizer=self.tokenizer,
            max_length=self.max_length,
            device=self.device,
            type_ontology=type_ontology,
            config=self.config
        )
    
    def convert_single_column_instances_to_dataset(self, single_column_instances: List[SingleColumnPllInstance], type_ontology: List[str]):
        """
        Convert single column PLL instances to dataset format compatible with doduo_single trainer.
        
        Args:
            single_column_instances: List of SingleColumnPllInstance objects
            type_ontology: Available type vocabulary
            
        Returns:
            Dataset compatible with doduo_single trainer
        """
        logger.info(f"Converting {len(single_column_instances)} single column instances to dataset format...")
        
        # Create a custom dataset that extends SatoCVColwiseDataset        
        return SinglePLLDataset(
            single_column_instances, 
            self.tokenizer, 
            type_ontology,
            max_length=self.max_length,
            device=self.device,
            config=self.config
        )

# src/dataset/data_converters_proden.py

import torch
import numpy as np
import pandas as pd
import operator
from functools import reduce
from tqdm import tqdm
import logging

logger = logging.getLogger(__name__)


class MultiPLLDataset:
    """
    Multi-column PLL Dataset for ProDEN with Augmentation (Back-Translation / Shuffle+Dropout).

    Changes:
      - Added input_ids_bt (augment view): Shuffle and Dropout on column elements.
      - Added cls_indexes_bt: CLS positions corresponding to augmented views.
    """

    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config, threshold_ratio=0.01):
        self.type_ontology = type_ontology

        if all(hasattr(inst, "table") for inst in instances):
            instances = self._convert_table_instances(instances)

        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.model_name = config.model_name.lower()

        # Whether to enable AUM mode
        self.use_aum = bool(getattr(config, "use_aum", False) and threshold_ratio > 0)
        print(f"[AUM] Use AUM mode: {self.use_aum}")

        # ==========================================================
        # Number of classes +1 (Reserve a "Noise" class for AUM)
        self.num_real_classes = len(type_ontology)
        self.noise_class_id = self.num_real_classes if self.use_aum else None
        self.threshold_ratio = threshold_ratio if self.use_aum else 0.0
        # self.num_classes = self.num_real_classes + (1 if self.use_aum else 0)
        # ==========================================================

        # Group by table_id
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)

        data_list = []
        global_col_counter = 0  # Global column counter

        # Record which IDs were selected as threshold samples for subsequent analysis
        self.threshold_sample_ids = []
        
        for table_id, table_pll_instances in tqdm(table_groups.items(), desc="Building MultiPLL Dataset"):
            # Maintain column order
            table_pll_instances.sort(key=lambda x: x.col_idx)

            if len(table_pll_instances) > 10:
                continue

            token_ids_list = []
            token_ids_list_bt = []
            candidate_labels_per_col = []
            weak_label_ids = []
            class_ids = []
            col_indices = []
            global_col_indices = []

            is_threshold_flags = []

            for pll_instance in table_pll_instances:
                # 1. Get original column data (List)
                if hasattr(pll_instance.column_features, 'tolist'):
                    col_data_raw = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    col_data_raw = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    col_data_raw = [str(pll_instance.column_features)]

                # 2. Generate original view tokens
                tokens_orig = self._get_token_ids(col_data_raw, augment=False)
                token_ids_list.append(tokens_orig)

                # 3. Generate augmented view tokens (Shuffle + Dropout) -> input_ids_bt
                #    Note: Augmented view is only used for consistency loss calculation, not for GT label matching,
                #          but the order must correspond to columns
                tokens_bt = self._get_token_ids(col_data_raw, augment=True)
                token_ids_list_bt.append(tokens_bt)

                # 4. Label processing (unchanged)
                cand_ids = [self._label_to_class_id(label) for label in getattr(pll_instance, "candidate_set", []) if label is not None]
                if len(cand_ids) == 0:
                    # Fallback: If no candidates, fill with true value to avoid empty weak labels
                    cand_ids = [self._label_to_class_id(pll_instance.column_name)]
                candidate_labels_per_col.append(cand_ids)

                true_class_id = self._label_to_class_id(pll_instance.column_name)
                weak_class_id = cand_ids[0]
                # ==========================================================
                # Decide whether to use as Threshold Sample (enabled only in AUM mode)
                # Logic: If threshold sample, force GT to Noise Class
                is_threshold = False
                if self.use_aum and self.threshold_ratio > 0 and random.random() < self.threshold_ratio:
                    weak_class_id = self.noise_class_id # Force to N+1 class
                    is_threshold = True
                    self.threshold_sample_ids.append(global_col_counter)
                
                weak_label_ids.append(weak_class_id)
                class_ids.append(true_class_id)
                is_threshold_flags.append(1 if is_threshold else 0)
                # ==========================================================
                
                col_indices.append(pll_instance.col_idx)
                global_col_indices.append(global_col_counter)
                global_col_counter += 1

            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)
            cls_indexes = self._calculate_cls_indexes(token_ids_list).to(device)
            token_ids_bt = torch.LongTensor(reduce(operator.add, token_ids_list_bt)).to(device)
            cls_indexes_bt = self._calculate_cls_indexes(token_ids_list_bt).to(device)

            weak_label_tensor = torch.LongTensor(weak_label_ids).to(device)
            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            col_indices_tensor = torch.LongTensor(col_indices).to(device)
            global_col_indices_tensor = torch.LongTensor(global_col_indices).to(device)
            is_threshold_tensor = torch.LongTensor(is_threshold_flags).to(device)   # Added

            data_list.append([
                table_id,
                len(table_pll_instances),
                token_ids,              # input_ids
                token_ids_bt,           # input_ids_bt [new]
                weak_label_tensor,
                class_ids_tensor,
                cls_indexes,            # cls_indexes
                cls_indexes_bt,         # cls_indexes_bt [new]
                candidate_labels_per_col,
                col_indices_tensor,
                global_col_indices_tensor,
                is_threshold_tensor # [new] for Trainer to distinguish
            ])
        
        self.table_df = pd.DataFrame(
            data_list,
            columns=[
                "table_id",
                "num_col",
                "data_tensor",      # input_ids
                "data_tensor_bt",   # input_ids_bt [new]
                "weak_label_tensor",
                "label_tensor",
                "cls_indexes",
                "cls_indexes_bt",   # [new]
                "candidate_labels",
                "col_indices",
                "global_col_indices",
                "is_threshold" # [new] for Trainer to distinguish
            ],
        )

        # Record total samples for Trainer
        self.num_samples = global_col_counter
        print(f"[Dataset] Built with {len(self.table_df)} tables, {global_col_counter} cols.")
        if self.use_aum:
            print(f"[AUM] Created {len(self.threshold_sample_ids)} threshold samples (Label={self.noise_class_id}).")

    def _get_token_ids(self, col_data_list: List[str], augment: bool = False) -> List[int]:
        """
        Serialize and tokenize.
        Augment strategy: Random Shuffle + 15% Dropout
        """
        # Copy data to avoid modifying the original reference
        vals = col_data_list[:]

        if augment and len(vals) > 0:
            # 1. Shuffle
            random.shuffle(vals)
            # 2. Dropout (keep 85%)
            if len(vals) > 1:
                keep_n = max(1, int(len(vals) * 0.85))
                vals = vals[:keep_n]

        col_str = " ".join(vals) if vals else ""

        # Tokenize
        token_ids = self.tokenizer.encode(
            col_str,
            add_special_tokens=True, # Automatically add [CLS] ... [SEP]
            max_length=self.max_length + 2,
            truncation=True
        )
        return token_ids

    def _calculate_cls_indexes(self, token_ids_list: List[List[int]]) -> torch.Tensor:
        """Calculate CLS positions based on token list lengths"""
        lengths = np.array([len(x) for x in token_ids_list])
        
        if "bert" in self.model_name:
            # BERT: CLS at the beginning
            # [CLS] A [SEP], [CLS] B [SEP] ...
            # 0, len(A), len(A)+len(B)...
            cls_indices = [0] + np.cumsum(lengths).tolist()[:-1]
            return torch.LongTensor(cls_indices)

        elif "qwen" in self.model_name:
            # Causal LM: Pooling usually takes the last token
            # len(A)-1, len(A)+len(B)-1 ...
            last_indices = np.cumsum(lengths) - 1
            return torch.LongTensor(last_indices)
        else:
            # Default to BERT processing
            cls_indices = [0] + np.cumsum(lengths).tolist()[:-1]
            return torch.LongTensor(cls_indices)

    def _label_to_class_id(self, label):
        # Allow label to be str or int (index)
        if isinstance(label, (int, np.integer)):
            return int(label) if 0 <= int(label) < len(self.type_ontology) else 0
        try:
            label_index = self.type_ontology.index(str(label))
            return label_index
        except ValueError:
            # If label is a numeric string, try to parse it
            if isinstance(label, str) and label.isdigit():
                idx = int(label)
                return idx if 0 <= idx < len(self.type_ontology) else 0
            return 0

    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]

        return {
            "data": row["data_tensor"],              # input_ids
            "data_bt": row["data_tensor_bt"],        # input_ids_bt [new]
            "weak_labels": row["weak_label_tensor"].flatten(),
            "labels": row["label_tensor"].flatten(),
            "is_threshold": row["is_threshold"].flatten(),
            "table_id": row["table_id"],
            "col_idx": row["col_indices"],
            "global_col_indices": row["global_col_indices"],
            "cls_indexes": row["cls_indexes"],
            "cls_indexes_bt": row["cls_indexes_bt"], # [new]
            "candidate_labels": row["candidate_labels"],
        }

    def _convert_table_instances(self, table_instances: List[TableInstance]) -> List[PllDataInstance]:
        """Convert TableInstance to PllDataInstance for consistent processing with training data."""
        pll_list = []
        for table_instance in table_instances:
            table_df = table_instance.table
            labels = getattr(table_instance, "labels", {}) or {}
            col_indices = getattr(table_instance, "col_indices", None)

            # If col_indices not provided, generate sequentially
            if not col_indices or len(col_indices) != len(table_df.columns):
                col_indices = list(range(len(table_df.columns)))

            for idx, col_name in enumerate(table_df.columns):
                label_val = labels.get(col_name)
                label_text = self._normalize_label_value(label_val)
                # If label is missing, use column name as placeholder to avoid empty candidates
                column_label = label_text if label_text is not None else col_name
                candidate_set = [column_label] if column_label is not None else []

                pll_list.append(PllDataInstance(
                    table_id=table_instance.table_id,
                    column_name=column_label,
                    column_features=table_df[col_name],
                    candidate_set=candidate_set,
                    col_idx=col_indices[idx]
                ))
        return pll_list

    def _normalize_label_value(self, label_val):
        """Normalize label value to ontology name, supporting numeric index/string."""
        if label_val is None:
            return None
        # Numeric index
        if isinstance(label_val, (int, np.integer)):
            return self.type_ontology[label_val] if 0 <= label_val < len(self.type_ontology) else str(label_val)
        # String index
        if isinstance(label_val, str) and label_val.isdigit():
            idx = int(label_val)
            return self.type_ontology[idx] if 0 <= idx < len(self.type_ontology) else label_val
        return str(label_val)




class _MultiPLLDataset:
    """
    Multi-column PLL Dataset for ProDEN.

    Basically consistent with the original MultiPLLDataset, differences:
      - Remove candidate_distributions / candidate_mask logic used by PoP;
      - Additionally assign a global column index global_col_indices for each column to facilitate Trainer's global Q maintenance.
    """

    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config):
        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.type_ontology = type_ontology

        # For counting total columns
        self.num_classes = len(type_ontology)

        # Group by table_id
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)

        data_list = []
        global_col_counter = 0  # Global column counter

        for table_id, table_pll_instances in tqdm(table_groups.items(), desc="Building ProDEN MultiPLLDataset"):
            # Maintain column order
            table_pll_instances.sort(key=lambda x: x.col_idx)

            # Your original code had logic to skip when "len(table_pll_instances) > 10" to avoid OOM, keeping it here
            if len(table_pll_instances) > 10:
                continue

            token_ids_list = []
            candidate_labels_per_col = []
            class_ids = []
            col_indices = []
            global_col_indices = []

            for pll_instance in table_pll_instances:
                # Construct column value text
                if hasattr(pll_instance.column_features, 'tolist'):
                    col_data = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    col_data = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    col_data = [str(pll_instance.column_features)]
                col_str = " ".join(col_data) if col_data else ""

                # tokenize
                token_ids = tokenizer.encode(
                    col_str,
                    add_special_tokens=True,
                    max_length=max_length + 2,
                    truncation=True
                )
                token_ids_list.append(token_ids)

                # Candidate labels -> class id
                cand_ids = [self._label_to_class_id(label) for label in pll_instance.candidate_set]
                candidate_labels_per_col.append(cand_ids)

                # True label (for eval)
                true_class_id = self._label_to_class_id(pll_instance.column_name)
                class_ids.append(true_class_id)

                # Current column's col_idx in the table
                col_indices.append(pll_instance.col_idx)

                # Assign global column index
                global_col_indices.append(global_col_counter)
                global_col_counter += 1

            # Concatenate tokens for all columns in the same table
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)

            # CLS positions (selected based on your config's model type)
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]

            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1

            if "bert" in config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(device)
            elif "qwen" in config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(device)
            else:
                raise ValueError(f"Unsupported model: {config.model_name}")

            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            col_indices_tensor = torch.LongTensor(col_indices).to(device)
            global_col_indices_tensor = torch.LongTensor(global_col_indices).to(device)
            
            data_list.append([
                table_id,
                len(table_pll_instances),   # num_col
                token_ids,
                class_ids_tensor,
                cls_indexes,
                candidate_labels_per_col,
                col_indices_tensor,
                global_col_indices_tensor,
            ])

        self.table_df = pd.DataFrame(
            data_list,
            columns=[
                "table_id",
                "num_col",
                "data_tensor",
                "label_tensor",
                "cls_indexes",
                "candidate_labels",
                "col_indices",
                "global_col_indices",
            ],
        )

        logger.info(f"[ProDEN] Built dataset with {len(self.table_df)} tables, "
                    f"{global_col_counter} columns total.")

    def _label_to_class_id(self, label):
        """Convert label string to class ID using type ontology."""
        try:
            label_index = self.type_ontology.index(label)
            return label_index
        except ValueError:
            logger.warning(f"Label '{label}' not found in type ontology, using default ID 0")
            return 0

    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]

        return {
            "data": row["data_tensor"],
            "label": row["label_tensor"].flatten(),          # [num_cols]
            "table_id": row["table_id"],
            "col_idx": row["col_indices"],                  # [num_cols]
            "global_col_indices": row["global_col_indices"],# [num_cols] global column indices
            "cls_indexes": row["cls_indexes"],              # [num_cols]
            "candidate_labels": row["candidate_labels"],    # List[List[int]]
        }

class _MultiPLLDataset:
    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config):
        self.pll_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.device = device
        self.type_ontology = type_ontology
        # Store progressively refined soft candidate distributions per (table_id, col_idx)
        # Value shape: [num_classes] float tensor on CPU
        self._candidate_distributions = {}
        
        # Group PLL instances by table_id
        table_groups = {}
        for pll_instance in instances:
            table_id = pll_instance.table_id
            if table_id not in table_groups:
                table_groups[table_id] = []
            table_groups[table_id].append(pll_instance)
        
        # Convert grouped PLL instances to DataFrame format
        data_list = []
        for table_id, table_pll_instances in tqdm(table_groups.items()):
            # Sort by col_idx to maintain order
            table_pll_instances.sort(key=lambda x: x.col_idx)
            
            # Avoid OOM
            if len(table_pll_instances) > 10:
                continue
            
            # Tokenize table data
            token_ids_list = []
            candidate_labels = []
            class_ids = []
            col_indices = []
            
            for pll_instance in table_pll_instances:
                # Get column data from column_features
                if hasattr(pll_instance.column_features, 'tolist'):
                    # If it's a pandas Series
                    col_data = pll_instance.column_features.dropna().astype(str).tolist()
                elif isinstance(pll_instance.column_features, list):
                    # If it's already a list
                    col_data = [str(x) for x in pll_instance.column_features if pd.notna(x)]
                else:
                    # If it's a string representation
                    col_data = [str(pll_instance.column_features)]
                
                col_str = " ".join(col_data) if col_data else ""
                
                token_ids = tokenizer.encode(
                    col_str, 
                    add_special_tokens=True, 
                    max_length=max_length + 2
                )
                token_ids_list.append(token_ids)
                
                # Convert candidate labels to class IDs
                candidate_class_ids = [self._label_to_class_id(label) for label in pll_instance.candidate_set]
                candidate_labels.append(candidate_class_ids)
                
                # Use column_name as the true label
                true_class_id = self._label_to_class_id(pll_instance.column_name)
                class_ids.append(true_class_id)
                
                # Collect col_idx
                col_indices.append(pll_instance.col_idx)
            
            # Combine all token IDs
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(device)
            
            # Calculate CLS indexes
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]

            # Calculate last token index
            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1
            
            if "bert" in config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(device)
            elif "qwen" in config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(device)
            else:
                raise ValueError(f"Unsupported model: {config.model_name}")
            
            # cls_indexes = torch.LongTensor(cls_index_list).to(device)
            class_ids_tensor = torch.LongTensor(class_ids).to(device)
            
            data_list.append([
                table_id,
                len(table_pll_instances),  # num_col
                token_ids,
                class_ids_tensor,
                cls_indexes,
                candidate_labels,
                col_indices,
            ])
        
        # Create DataFrame in SatoCVTablewiseDataset format
        self.table_df = pd.DataFrame(
            data_list,
            columns=["table_id", "num_col", "data_tensor", "label_tensor", "cls_indexes", "candidate_labels", "col_indices"]
        )
    
    def update_candidate_masks(self, updates: list) -> None:
        """Update internal soft candidate distributions with PRODEN-style refined masks.
        Each update item is a dict with keys: 'table_id', 'col_idx', 'updated_mask' (Tensor [C]).
        """
        if updates is None:
            return
        for item in updates:
            table_id = item.get('table_id')
            col_idx = int(item.get('col_idx')) if item.get('col_idx') is not None else None
            updated_mask = item.get('updated_mask')
            if table_id is None or col_idx is None or updated_mask is None:
                continue
            # Ensure CPU float tensor
            if isinstance(updated_mask, torch.Tensor):
                mask = updated_mask.detach().to('cpu').float()
            else:
                mask = torch.tensor(updated_mask, dtype=torch.float32)
            self._candidate_distributions[(table_id, col_idx)] = mask
    
    def get_candidate_distribution(self, table_id: str, col_idx: int):
        """Return refined soft distribution for (table_id, col_idx) if available, else None."""
        return self._candidate_distributions.get((table_id, int(col_idx)))
    
    def _label_to_class_id(self, label):
        """Convert label string to class ID using type ontology."""
        # Use the loaded type ontology to map labels to IDs
        try:
            # Find the index of the label in the type ontology
            label_index = self.type_ontology.index(label)
            return label_index
        except ValueError:
            # If label not found in ontology, return 0 (default)
            logger.warning(f"Label '{label}' not found in type ontology, using default ID 0")
            return 0
    
    def __len__(self) -> int:
        return len(self.table_df)

    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]
        col_indices = torch.LongTensor(row["col_indices"])
        table_id = row["table_id"]
        
        # get soft candidate mask dynamically
        refined_mask = []
        for i, col_idx in enumerate(col_indices.tolist()):
            soft_mask = self.get_candidate_distribution(table_id, col_idx)
            if soft_mask is not None:
                refined_mask.append(soft_mask)
            else:
                mask = torch.zeros(len(self.type_ontology))
                for label in row["candidate_labels"][i]:
                    mask[label] = 1
                refined_mask.append(mask)
        refined_mask = torch.stack(refined_mask)
        
        return {
            "data": row["data_tensor"],
            "label": row["label_tensor"].flatten(),
            "table_id": table_id,
            "col_idx": col_indices,
            "cls_indexes": row["cls_indexes"],
            "candidate_labels": row["candidate_labels"],
            "candidate_mask": refined_mask
        }
    
    # def __getitem__(self, idx: int) -> dict:
    #     row = self.table_df.iloc[idx]
    #     # Use the saved col_idx from PLL instances instead of manually calculating
    #     col_indices = torch.LongTensor(row["col_indices"])
        
    #     return {
    #         "data": row["data_tensor"],
    #         "label": row["label_tensor"].flatten(),  # Flatten multi-column labels
    #         "table_id": row["table_id"],
    #         "candidate_labels": row["candidate_labels"],
    #         "col_idx": col_indices,  # Column indices from original data
    #         "cls_indexes": row["cls_indexes"]
    #     }

class SinglePLLDataset(SatoCVColwiseDataset):
    def __init__(self, single_column_instances, tokenizer, type_ontology, max_length=128, device=None, config=None):
        self.single_column_instances = single_column_instances
        self.tokenizer = tokenizer
        self.type_ontology = type_ontology
        self.max_length = max_length
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device
        
        # Process single column instances into the format expected by SatoCVColwiseDataset
        self._process_single_column_instances()
    
    def _process_single_column_instances(self):
        """Process single column PLL instances into SatoCVColwiseDataset format"""
        
        row_list = []
        for instance in self.single_column_instances:
            # Convert candidate labels to label IDs
            candidate_label_ids = []
            for label in instance.candidate_labels:
                if label in self.type_ontology:
                    label_id = self.type_ontology.index(label)
                    candidate_label_ids.append(label_id)
            
            # Get true label ID
            true_label_id = 0
            if instance.column_name in self.type_ontology:
                true_label_id = self.type_ontology.index(instance.column_name)
            
            # Create row data similar to SatoCVColwiseDataset format
            row_data = {
                'table_id': instance.table_id,
                'col_idx': instance.col_idx,
                'col_name': instance.column_name,
                'class_id': true_label_id,
                'data': instance.column_data,
                'candidate_labels': candidate_label_ids  # Add candidate labels
            }
            row_list.append(row_data)
        
        # Create DataFrame
        self.df = pd.DataFrame(row_list)
        
        # Convert into torch.Tensor (similar to SatoCVColwiseDataset)
        self.df["data_tensor"] = self.df["data"].apply(
            lambda x: torch.LongTensor(
                self.tokenizer.encode(x,
                                    add_special_tokens=True,
                                    max_length=self.max_length + 2)).to(self.device))
        self.df["label_tensor"] = self.df["class_id"].apply(
            lambda x: torch.LongTensor([x]).to(self.device)
        )
        # Store progressively refined soft candidate distributions per (table_id, col_idx)
        # Value shape: [num_classes] float tensor on CPU
        self._candidate_distributions = {}
    
    def __len__(self) -> int:
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        return {
            'data': row["data_tensor"],  # Use 'data' key for compatibility with doduo_single_trainer
            'label': row["label_tensor"],
            'table_id': row["table_id"],
            'col_idx': row["col_idx"],
            'col_name': row["col_name"],
            'candidate_labels': row["candidate_labels"]  # Include candidate labels
        }
    
    def update_candidate_masks(self, updates: list) -> None:
        """Update internal soft candidate distributions with PRODEN-style refined masks.
        Each update item is a dict with keys: 'table_id', 'col_idx', 'updated_mask' (Tensor [C]).
        """
        if updates is None:
            return
        for item in updates:
            table_id = item.get('table_id')
            col_idx = int(item.get('col_idx')) if item.get('col_idx') is not None else None
            updated_mask = item.get('updated_mask')
            if table_id is None or col_idx is None or updated_mask is None:
                continue
            # Ensure CPU float tensor
            if isinstance(updated_mask, torch.Tensor):
                mask = updated_mask.detach().to('cpu').float()
            else:
                mask = torch.tensor(updated_mask, dtype=torch.float32)
            self._candidate_distributions[(table_id, col_idx)] = mask

    def get_candidate_distribution(self, table_id: str, col_idx: int):
        """Return refined soft distribution for (table_id, col_idx) if available, else None."""
        return self._candidate_distributions.get((table_id, int(col_idx)))

# Create a custom dataset that wraps SatoCVTablewiseDataset
# but uses LLM data instead of loading from CSV
class TablewiseDataset:
    def __init__(self, instances, tokenizer, max_length, device, type_ontology, config=None):
        self.table_instances = instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.type_ontology = type_ontology
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device

        self._process_table_instances()

    def _label_to_class_id(self, label):
        label_index = self.type_ontology.index(label)
        return label_index

    def _process_table_instances(self):        
        data_list = []
        for table_instance in tqdm(self.table_instances):
            table_df = table_instance.table
            labels = table_instance.labels

            # Avoid OOM
            if len(table_instance.table.columns) > 10:
                continue
            
            # Handle col_indices - use saved ones if available, otherwise generate them
            if hasattr(table_instance, 'col_indices') and table_instance.col_indices is not None:
                col_indices = table_instance.col_indices
            else:
                # Generate column indices for tables without saved col_indices
                col_indices = list(range(len(table_df.columns)))
            
            # Convert table data to token IDs (similar to SatoCVTablewiseDataset)
            token_ids_list = []
            candidate_labels = []
            for col_name in table_df.columns:
                # Get column data and convert to string
                try:
                    col_data = table_df[col_name].dropna().astype(str).tolist()
                except:
                    print(table_df[col_name])
                col_str = " ".join(col_data) if col_data else ""
                
                token_ids = self.tokenizer.encode(
                    col_str, 
                    add_special_tokens=True, 
                    max_length=self.max_length + 2
                )
                token_ids_list.append(token_ids)
                # Convert candidate labels to class IDs
                # candidate_class_ids = [self._label_to_class_id(label) for label in table_instance["candidate_set"]]
                # candidate_labels.append(candidate_class_ids)
            
            # Combine all token IDs
            token_ids = torch.LongTensor(reduce(operator.add, token_ids_list)).to(self.device)
            
            # Calculate CLS indexes
            cls_index_list = [0] + np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ).tolist()[:-1]
            
            last_token_index_list = np.cumsum(
                np.array([len(x) for x in token_ids_list])
            ) - 1
            
            if "bert" in self.config.model_name.lower():
                cls_indexes = torch.LongTensor(cls_index_list).to(self.device)
            elif "qwen" in self.config.model_name.lower():
                cls_indexes = torch.LongTensor(last_token_index_list).to(self.device)
            else:
                raise ValueError(f"Unsupported model: {self.config.model_name}")
            
            # Convert labels to class IDs
            # labels is a dict mapping column names to class IDs
            class_ids = torch.LongTensor([labels.get(col_name, 0) for col_name in table_df.columns]).to(self.device)
            
            data_list.append([
                table_instance.table_id,
                len(table_df.columns),  # num_col
                token_ids,
                class_ids,
                cls_indexes,
                # candidate_labels,
                col_indices
            ])
        
        # Create DataFrame in SatoCVTablewiseDataset format
        self.table_df = pd.DataFrame(
            data_list,
            columns=["table_id", "num_col", "data_tensor", "label_tensor", "cls_indexes", 
            # "candidate_labels", 
            "col_indices"]
        )
    
    def __len__(self) -> int:
        return len(self.table_df)
    
    def __getitem__(self, idx: int) -> dict:
        row = self.table_df.iloc[idx]
        # Use the saved col_indices from original data instead of manually calculating
        col_indices = torch.LongTensor(row["col_indices"])
        
        return {
            "data": row["data_tensor"],
            "labels": row["label_tensor"],
            # "weak_labels": None,
            "table_id": row["table_id"],
            "col_idx": col_indices,  # Column indices from original data
            # "candidate_labels": row["candidate_labels"],  # Standard method doesn't use candidate labels
            "candidate_labels": None,
            "cls_indexes": row["cls_indexes"]
        }

class ColwiseDataset(SatoCVColwiseDataset):
    def __init__(self, table_instances, tokenizer, max_length=128, device=None, config=None):
        self.table_instances = table_instances
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.config = config
        if device is None:
            device = torch.device('cpu')
        self.device = device
        
        # Process table instances into the format expected by SatoCVColwiseDataset
        self._process_table_instances()

    def _process_table_instances(self):
        """Process LLM table instances into SatoCVColwiseDataset format"""
        
        row_list = []
        for table_instance in self.table_instances:
            table_df = table_instance.table
            labels = table_instance.labels
            
            # Process each column as a separate row
            for col_idx, col_name in enumerate(table_df.columns):
                # Get column data and convert to string
                col_data = table_df[col_name].dropna().astype(str).tolist()
                col_str = " ".join(col_data) if col_data else ""
                
                # Get label for this column
                label_id = labels.get(col_name, 0)
                
                # Create row data similar to SatoCVColwiseDataset format
                row_data = {
                    'table_id': table_instance.table_id,
                    'col_idx': col_idx,
                    'col_name': col_name,
                    'class_id': label_id,
                    'data': col_str
                }
                row_list.append(row_data)
        
        # Create DataFrame
        self.df = pd.DataFrame(row_list)
        
        # Convert into torch.Tensor (similar to SatoCVColwiseDataset)
        self.df["data_tensor"] = self.df["data"].apply(
            lambda x: torch.LongTensor(
                self.tokenizer.encode(x,
                                    add_special_tokens=True,
                                    max_length=self.max_length + 2)).to(self.device))
        self.df["label_tensor"] = self.df["class_id"].apply(
            lambda x: torch.LongTensor([x]).to(self.device)
        )

    def __len__(self) -> int:
        return len(self.df)
    
    def __getitem__(self, idx: int) -> dict:
        return {
            "data": self.df.iloc[idx]["data_tensor"],
            "label": self.df.iloc[idx]["label_tensor"],
            "candidate_labels": None  # Single column method doesn't use candidate labels
        }